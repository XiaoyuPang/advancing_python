爬虫学习

	1：发送requests并拿到 HTML response		
	        python第三方库requests模块
	2：解析response	
		    re(正则）、xpath和css（对应XML文件，使用lxml库）、DOM（beautifulsoup）、jsonpath,pyquery等
	3:如何处理动态HTML,验证码。
	        通用的动态页面采集：selenium + phantomjs 模拟真真实的浏览器加载js
	         Tesseract：机器学习库，机器图像识别系统，可以处理简单的验证码（数字或字符类），复杂类的只能“人工打码”
	4:scrapy框架：（scrapy，pyspider）
	        高性能高定制（异步网络框架twisted），所以下载速度非常快
        5：分布式策略：scrapy-redis
            scrapy-redis，在scrapy的基础上添加redis数据库为核心的组件，让scrapy框架支持分布式的功能
        6：反爬虫
            user-agent、代理、验证码、动态加载数据、加密数据

小结：
    1.爬虫最关键的不是页面信息，而是数据来源
    2.获取json数据：
        以ajax方式加载的页面，数据来源一定是json，不要从页面中死磕抓取json数据，
        以抓包方式找到后台传输数据的ajax地址，post或get就获得json数据了。
    3.让程序忽略SSL证书验证：
        如12306网站提供的SSL证书是自己做的，未经过CA认证，程序访问会报错。
        r = requests(url,verify=False) requests库的requests对象参数verify=False就忽略验证。
    4.处理乱码：
        拿到相应r后，最好先转码，一般网站都是utf-8，r.encoding="utf-8"
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
